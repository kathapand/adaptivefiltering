{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50d2aed0",
   "metadata": {},
   "source": [
    "## Working with filter pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5e6737",
   "metadata": {},
   "source": [
    "This Jupyter notebook explains the workflow of setting up and configuring a ground point filtering pipeline. This is an advanced workflow for users that want to define their own filtering workflows. For basic use, preconfigured pipelines are (or rather: will be) provided by `adaptivefiltering`. As always, we first need to import our library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a572cc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import adaptivefiltering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62895679",
   "metadata": {},
   "source": [
    "Also, we need to load at least one data set which we will use to interactively preview our filter settings. Note that for a good interactive experience with no downtimes, you should restrict your datasets to a reasonable size (see the [Working with datasets](datasets.ipynb) notebook for how to do it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171b20ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = adaptivefiltering.DataSet(filename=\"data/500k_NZ20_Westport.laz\")\n",
    "dataset = adaptivefiltering.DataSet(filename=\"data/uls_thingstaette.las\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0350633f",
   "metadata": {},
   "source": [
    "### Filtering backends"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c57133c",
   "metadata": {},
   "source": [
    "`adaptivefiltering` does not implement its own ground point filtering algorithms. Instead, algorithms from existing packages are accessible through a common interface. Currently, the following backends are available:\n",
    "\n",
    "* [PDAL](https://pdal.io/): The Point Data Abstraction Library is an open source library for point cloud processing.\n",
    "* [OPALS](https://opals.geo.tuwien.ac.at/html/stable/index.html) is a proprietary library for processing Lidar data. It can be tested freely for datasets <1M points.\n",
    "\n",
    "PDAL is always available when using `adaptivefiltering` and is used internally for many tasks that are not directly related to ground point filtering. In order to enable the OPALS backend, `adaptivefiltering` needs to be given the information where your OPALS installation (potentially including your license key) is located. This can either be done by setting the environment variable `OPALS_DIR` or by setting the path at runtime:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac76b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptivefiltering.set_opals_directory(\"/path/to/opals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2eb2724",
   "metadata": {},
   "source": [
    "### Configuring a filter pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26a84f8",
   "metadata": {},
   "source": [
    "The main pipeline configuration is done by calling the `pipeline_tuning` function with your dataset as the parameter. This will open the interactive user interface which waits for your user input until you hit the *Finalize* button. The configured filter is then accessible as the Python object `pipeline`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61c6a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = adaptivefiltering.pipeline_tuning(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8b8676",
   "metadata": {},
   "source": [
    "If you want to inspect multiple data sets in parallel while tuning a pipeline, you can do so by passing a list of datasets to the `pipeline_tuning` function. Note that `adaptivefiltering` does currently not parallelize the execution of filter pipeline execution which may have a negative impact on wait times while tuning with multiple parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8901f797",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline2 = adaptivefiltering.pipeline_tuning(datasets=[dataset, dataset])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f2397f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Storing and reloading filter pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949466de",
   "metadata": {},
   "source": [
    "Pipeline objects can be stored on disk with the `save_filter` command from `adaptivefiltering`. We typically use the extension `json` for filters. It stands for *JavaScript Object Notation* and is a widely used format for storing custom data structures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554ffebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptivefiltering.save_filter(pipeline, \"myfilter.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf308765",
   "metadata": {},
   "source": [
    "The appropriate counterpart is `load_filter`, which restores the pipeline object from a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e437e2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_pipeline = adaptivefiltering.load_filter(\"myfilter.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525d0824",
   "metadata": {},
   "source": [
    "A filter pipeline loaded from a file can be edited using the `pipeline_tuning` command by passing it to the function. As always, the pipeline object returned by `pipeline_tuning` will be a new object - no implicit changes of the loaded pipeline object will occur:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6e7bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "edited_pipeline = adaptivefiltering.pipeline_tuning(dataset, pipeline=old_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba37683",
   "metadata": {},
   "source": [
    "### Applying filter pipelines to data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64361c0b",
   "metadata": {},
   "source": [
    "Pipeline objects can also be used to transform data sets by applying the ground point filtering algorithms. This is one of the core tasks of the `adaptivefiltering` library, but this will rarely be done in this manual fashion, as we will provide additional interfaces for (locally adaptive) application of filter pipelines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2353933",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = pipeline.execute(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd89e8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d675111c55b50e49d58e5179e69cc710d4110717fd214fa9e6424bfa118e8c2b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('adaptivefiltering': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
