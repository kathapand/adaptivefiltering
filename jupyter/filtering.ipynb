{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a757ab54-e828-4e65-b373-f299df705561",
   "metadata": {},
   "source": [
    "## Working with filter pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b20ed5-296d-44d5-8260-2f44468f081e",
   "metadata": {},
   "source": [
    "This Jupyter notebook explains the workflow of setting up and configuring a ground point filtering pipeline. This is an advanced workflow for users that want to define their own filtering workflows. For basic use, preconfigured pipelines are (or rather: will be) provided by `adaptivefiltering`. As always, we first need to import our library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc43d668-e991-4a4c-9b74-68fe4caea542",
   "metadata": {},
   "outputs": [],
   "source": [
    "import adaptivefiltering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a62c72c-c761-4f46-b532-a91664c548e5",
   "metadata": {},
   "source": [
    "Also, we need to load at least one data set which we will use to interactively preview our filter settings. Note that for a good interactive experience with no downtimes, you should restrict your datasets to a reasonable size (see the [Working with datasets](datasets.ipynb) notebook for how to do it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871ac067-09d8-47fd-afde-f52c8cf75688",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = adaptivefiltering.DataSet(filename=\"data/500k_NZ20_Westport.laz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418af99f-61cc-4a62-a06a-b05a9fc1be32",
   "metadata": {},
   "source": [
    "### Filtering backends"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8983efd5-0539-4916-a01b-22c0575524c4",
   "metadata": {},
   "source": [
    "`adaptivefiltering` does not implement its own ground point filtering algorithms. Instead, algorithms from existing packages are accessible through a common interface. Currently, the following backends are available:\n",
    "\n",
    "* [PDAL](https://pdal.io/): The Point Data Abstraction Library is an open source library for point cloud processing.\n",
    "* [OPALS](https://opals.geo.tuwien.ac.at/html/stable/index.html) is a proprietary library for processing Lidar data. It can be tested freely for datasets <1M points.\n",
    "\n",
    "PDAL is always available when using `adaptivefiltering` and is used internally for many tasks that are not directly related to ground point filtering. In order to enable the OPALS backend, `adaptivefiltering` needs to be given the information where your OPALS installation (potentially including your license key) is located. This can either be done by setting the environment variable `OPALS_DIR` or by setting the path at runtime:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab717d2-673d-426d-941e-418e6cc9fb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptivefiltering.set_opals_directory(\"/path/to/opals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd3d013-5aa2-429f-bce4-3ad97f4fa759",
   "metadata": {},
   "source": [
    "### Configuring a filter pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c27405c-1b24-414b-ad80-a7bafc26c68a",
   "metadata": {},
   "source": [
    "The main pipeline configuration is done by calling the `pipeline_tuning` function with your dataset as the parameter. This will open the interactive user interface which waits for your user input until you hit the *Finalize* button. The configured filter is then accessible as the Python object `pipeline`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58282040-2b84-4e89-8634-f5e2d9d35f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = adaptivefiltering.pipeline_tuning(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cdec84-32d5-46af-a02a-d9edfced7a49",
   "metadata": {},
   "source": [
    "If you want to inspect multiple data sets in parallel while tuning a pipeline, you can do so by passing a list of datasets to the `pipeline_tuning` function. Note that `adaptivefiltering` does currently not parallelize the execution of filter pipeline execution which may have a negative impact on wait times while tuning with multiple parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940f323c-f044-49d9-8537-21ae01fd78e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline2 = adaptivefiltering.pipeline_tuning(datasets=[dataset, dataset])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c580d054-5862-428f-93fc-38a60ad8225b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Storing and reloading filter pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40092a0d-1c0f-4d18-922a-e4b63fb7cd8c",
   "metadata": {},
   "source": [
    "Pipeline objects can be stored on disk with the `save_filter` command from `adaptivefiltering`. We typically use the extension `json` for filters. It stands for *JavaScript Object Notation* and is a widely used format for storing custom data structures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc883453-05ff-4c03-afa1-182c71e8dfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptivefiltering.save_filter(pipeline, \"myfilter.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479e7868-d151-4220-b913-0f1a9b63fede",
   "metadata": {},
   "source": [
    "The appropriate counterpart is `load_filter`, which restores the pipeline object from a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af307a0-3779-455d-b49b-1fd16ab6facb",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_pipeline = adaptivefiltering.load_filter(\"myfilter.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e958d4-87a2-43d6-9bfa-5efcc8836f2f",
   "metadata": {},
   "source": [
    "A filter pipeline loaded from a file can be edited using the `pipeline_tuning` command by passing it to the function. As always, the pipeline object returned by `pipeline_tuning` will be a new object - no implicit changes of the loaded pipeline object will occur:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80f5f9c-7926-4b97-b16d-66a91958ccd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "edited_pipeline = adaptivefiltering.pipeline_tuning(dataset, pipeline=old_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e4e2f1-de83-48a8-b2a2-26a2fb268f14",
   "metadata": {},
   "source": [
    "### Applying filter pipelines to data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512c5d10-3f6d-4332-ad5f-fc98d155657f",
   "metadata": {},
   "source": [
    "Pipeline objects can also be used to transform data sets by applying the ground point filtering algorithms. This is one of the core tasks of the `adaptivefiltering` library, but this will rarely be done in this manual fashion, as we will provide additional interfaces for (locally adaptive) application of filter pipelines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802fba4c-44bb-4b83-89b9-3b378a28b71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = pipeline.execute(dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
