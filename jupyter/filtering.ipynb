{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a757ab54-e828-4e65-b373-f299df705561",
   "metadata": {},
   "source": [
    "## Configuring filter pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b20ed5-296d-44d5-8260-2f44468f081e",
   "metadata": {},
   "source": [
    "This Jupyter notebook explains the workflow of setting up and configuring a ground point filtering pipeline. This is an advanced workflow for users that want to define their own filtering workflows. For basic use, preconfigured pipelines are (or rather: will be) provided by `adaptivefiltering`. As always, we first need to import our library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc43d668-e991-4a4c-9b74-68fe4caea542",
   "metadata": {},
   "outputs": [],
   "source": [
    "import adaptivefiltering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a62c72c-c761-4f46-b532-a91664c548e5",
   "metadata": {},
   "source": [
    "Also, we need to load at least one data set which we will use to interactively preview our filter settings. Note that for a good interactive experience with no downtimes, you should restrict your datasets to a reasonable size (see the working with datasets demo notebook for how to do it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871ac067-09d8-47fd-afde-f52c8cf75688",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = adaptivefiltering.DataSet(filename=\"data/500k_NZ20_Westport.laz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c27405c-1b24-414b-ad80-a7bafc26c68a",
   "metadata": {},
   "source": [
    "The main pipeline configuration is done by calling the `pipeline_tuning` function with your dataset as the parameter. This will open the interactive user interface which waits for your user input until you hit the *Finalize* button. The configured filter is then accessible as the Python object `pipeline`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58282040-2b84-4e89-8634-f5e2d9d35f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = adaptivefiltering.pipeline_tuning(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40092a0d-1c0f-4d18-922a-e4b63fb7cd8c",
   "metadata": {},
   "source": [
    "This pipeline can e.g. be written to disk with the following command. We typically use the extension `json` for filters. It stands for *JavaScript Object Notation* and is a widely used format for storing custom data structures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc883453-05ff-4c03-afa1-182c71e8dfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptivefiltering.save_filter(pipeline, \"myfilter.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e958d4-87a2-43d6-9bfa-5efcc8836f2f",
   "metadata": {},
   "source": [
    "In the above, we covered only the basic use of the `pipeline_tuning` function. For other use cases, additional parameters can be passed. If you want to edit an existing ground point filtering pipeline, you can load it using the `load_filter` function and then pass it to `pipeline_tuning`. The pipeline object returned by `pipeline_tuning` will be a new object - no implicit changes of the loaded pipeline object will occur:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af307a0-3779-455d-b49b-1fd16ab6facb",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_pipeline = adaptivefiltering.load_filter(\"myfilter.json\")\n",
    "edited_pipeline = adaptivefiltering.pipeline_tuning(dataset, pipeline=old_pipeline)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
