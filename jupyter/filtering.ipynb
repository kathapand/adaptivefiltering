{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Configuring filter pipelines"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This Jupyter notebook explains the workflow of setting up and configuring a ground point filtering pipeline. This is an advanced workflow for users that want to define their own filtering workflows. For basic use, preconfigured pipelines are (or rather: will be) provided by `adaptivefiltering`. As always, we first need to import our library:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import adaptivefiltering"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Also, we need to load at least one data set which we will use to interactively preview our filter settings. Note that for a good interactive experience with no downtimes, you should restrict your datasets to a reasonable size (see the working with datasets demo notebook for how to do it)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "#dataset = adaptivefiltering.DataSet(filename=\"data/500k_NZ20_Westport.laz\")\n",
    "dataset = adaptivefiltering.DataSet(filename=\"data/uls_thingstaette.las\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The main pipeline configuration is done by calling the `pipeline_tuning` function with your dataset as the parameter. This will open the interactive user interface which waits for your user input until you hit the *Finalize* button. The configured filter is then accessible as the Python object `pipeline`:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "pipeline = adaptivefiltering.pipeline_tuning(dataset)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/home/gwydion/anaconda3/envs/adaptivefiltering/lib/python3.9/site-packages/adaptivefiltering/data/uls_thingstaette.las\n",
      "/home/gwydion/anaconda3/envs/adaptivefiltering/lib/python3.9/site-packages/adaptivefiltering/data/uls_thingstaette.las\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7f80665bc0884eec8062d56a3a8455a9"
      },
      "text/plain": [
       "AppLayout(children=(VBox(children=(HTML(value='Interactive pipeline configuration:', layout=Layout(grid_area='â€¦"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This pipeline can e.g. be written to disk with the following command. We typically use the extension `json` for filters. It stands for *JavaScript Object Notation* and is a widely used format for storing custom data structures:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "adaptivefiltering.save_filter(pipeline, \"myfilter.json\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the above, we covered only the basic use of the `pipeline_tuning` function. For other use cases, additional parameters can be passed. If you want to edit an existing ground point filtering pipeline, you can load it using the `load_filter` function and then pass it to `pipeline_tuning`. The pipeline object returned by `pipeline_tuning` will be a new object - no implicit changes of the loaded pipeline object will occur:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "old_pipeline = adaptivefiltering.load_filter(\"myfilter.json\")\n",
    "edited_pipeline = adaptivefiltering.pipeline_tuning(dataset, pipeline=old_pipeline)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you want to inspect multiple data sets in parallel while tuning a pipeline, you can do so by passing a list of datasets to the `pipeline_tuning` function. Note that `adaptivefiltering` does currently not parallelize the execution of filter pipeline execution which may have a negative impact on wait times while tuning with multiple parameters."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pipeline2 = adaptivefiltering.pipeline_tuning(datasets=[dataset, dataset])"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit ('adaptivefiltering': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "interpreter": {
   "hash": "d675111c55b50e49d58e5179e69cc710d4110717fd214fa9e6424bfa118e8c2b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}